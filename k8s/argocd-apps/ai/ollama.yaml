apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ollama
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: ai

  source:
    chart: ollama
    repoURL: https://helm.otwld.com/
    targetRevision: 1.37.0
    helm:
      releaseName: ollama
      values: |
        persistentVolume:
          enabled: true
          size: 50Gi
          storageClass: "proxmox-zfs-nvme"

        service:
          type: NodePort
          nodePort: 31434

        ollama:
          gpu:
            enabled: true
            type: nvidia
            number: 1
        
          # Models to pull at startup
          models:
            pull:
              - llama3.2:3b
              - nomic-embed-text
              - qwen3-coder:30b
        
          # Environment variables
          extraEnv:
            - name: OLLAMA_KEEP_ALIVE
              value: "24h"
            - name: OLLAMA_HOST
              value: "0.0.0.0"
        
        # Ingress (optional)
        ingress:
          enabled: false
          # enabled: true
          # className: nginx
          # annotations:
          #   cert-manager.io/cluster-issuer: letsencrypt-prod
          # hosts:
          #   - host: ollama.yourdomain.com
          #     paths:
          #       - path: /
          #         pathType: Prefix
          # tls:
          #   - secretName: ollama-tls
          #     hosts:
          #       - ollama.yourdomain.com

  destination:
    server: https://kubernetes.default.svc
    namespace: ai

  syncPolicy:
    automated:
      prune: true
      selfHeal: true